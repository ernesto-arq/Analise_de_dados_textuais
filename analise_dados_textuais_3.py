# -*- coding: utf-8 -*-
"""Analise_dados_textuais_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z_ccLZWPLGP1CckP_e0LFFJPSNd2qn_1

**Ernesto Gurgel Valente Neto**

---
# **Materia: Analise de Dados Textuais**


---

Prof: Wellington
"""

print('Importação de Pacotes')
import nltk
import re
nltk.download('gutenberg')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

"""1) 
# **Utililizando as técnicas aprendidas sobre Term Frequency, Inverse Document Frequency, liste as 5 palavras mais relevantes de cada texto contido no corpus Gutemberg. A sua analise deve ser em cima dos seguintes textos : 'austen-emma.txt', 'bible-kjv.txt', 'carroll-alice.txt', 'melville-moby_dick.txt', 'shakespeare-caesar.txt' e 'shakespeare-hamlet.txt'. Lembre-se que antes de aplicar o as tecnicas de TF IDF o texto deve está normalizado, com as stopwords retiradas e lematizado.**
"""

#nltk.corpus.gutenberg.fileids()
#Importando as bibliotecas
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize  
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
from nltk.corpus import gutenberg
#lista_stop = stopwords.words()
lista_stop = nltk.corpus.stopwords.words('english')

#tratando os dados
texto_hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')
texto_hamlet = set(word.lower() for word in texto_hamlet if word.isalpha())
texto_hamlet = [nltk.word_tokenize(str(comment)) for comment in texto_hamlet]

lista_stop
filtered_sentence_hamlet = []
for word in texto_hamlet:
    if word not in lista_stop:
        filtered_sentence_hamlet.append(word)

#tratando os dados
texto_emma = nltk.corpus.gutenberg.words('austen-emma.txt')
texto_emma = set(word.lower() for word in texto_emma if word.isalpha())
texto_emma = [nltk.word_tokenize(str(comment)) for comment in texto_emma]

lista_stop
filtered_sentence_emma = []
for word in texto_emma:
    if word not in lista_stop:
        filtered_sentence_emma.append(word)      

#tratando os dados
texto_bible = nltk.corpus.gutenberg.words('bible-kjv.txt')
texto_bible = set(word.lower() for word in texto_bible if word.isalpha())
texto_bible = [nltk.word_tokenize(str(comment)) for comment in texto_bible]

lista_stop
filtered_sentence_bible = []
for word in texto_bible:
    if word not in lista_stop:
        filtered_sentence_bible.append(word)

#tratando os dados
texto_alice = nltk.corpus.gutenberg.words('carroll-alice.txt')
texto_alice = set(word.lower() for word in texto_alice if word.isalpha())
texto_alice = [nltk.word_tokenize(str(comment)) for comment in texto_alice]

lista_stop
filtered_sentence_alice = []
for word in texto_alice:
    if word not in lista_stop:
        filtered_sentence_alice.append(word)

#tratando os dados
texto_moby = nltk.corpus.gutenberg.words('melville-moby_dick.txt')
texto_moby = set(word.lower() for word in texto_moby if word.isalpha())
texto_moby = [nltk.word_tokenize(str(comment)) for comment in texto_moby]

lista_stop
filtered_sentence_moby = []
for word in texto_moby:
    if word not in lista_stop:
        filtered_sentence_moby.append(word)

#tratando os dados
texto_caesar = nltk.corpus.gutenberg.words('shakespeare-caesar.txt')
texto_caesar = set(word.lower() for word in texto_caesar if word.isalpha())
texto_caesar = [nltk.word_tokenize(str(comment)) for comment in texto_caesar]

lista_stop
filtered_sentence_caesar = []
for word in texto_caesar:
    if word not in lista_stop:
        filtered_sentence_caesar.append(word)

#importando bibliotecas
import pandas as pd
import altair as alt

dfemma = filtered_sentence_emma
dfbiblia = filtered_sentence_bible
dfalice = filtered_sentence_alice
dfmoby = filtered_sentence_moby
dfcaesar = filtered_sentence_caesar
dfhamlet = filtered_sentence_hamlet
#print("Emma: ", len(dfemma), "/Biblia: ", len(dfbiblia), "/Alice: ",len(dfalice), "/Moby :", len(dfmoby), "/Caesar: ", len(dfcaesar), "/Hammlet: ", len(dfhamlet))

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

def normalize_text(txt):
    txt = txt.lower()
    txt = txt.replace(',','')
    txt = txt.replace(';','')
    txt = txt.replace('.','')
    txt = txt.replace('\'','')
    txt = txt.replace('(','')
    txt = txt.replace(')','')
    txt = txt.replace(':','')
    txt = txt.replace('!','')
    txt = txt.replace('?','')
    txt = txt.replace("\\","")
    txt = txt.replace("\"","")
    txt = txt.replace("`","")
    txt = txt.replace('</p>','')
    return txt

#Removendo caracteres especiais
normalize_text_emma = normalize_text(str(filtered_sentence_emma))
normalize_text_bible = normalize_text(str(filtered_sentence_bible))
normalize_text_alice = normalize_text(str(filtered_sentence_alice))
normalize_text_moby = normalize_text(str(filtered_sentence_moby))
normalize_text_caesar = normalize_text(str(filtered_sentence_caesar))
normalize_text_hamlet = normalize_text(str(filtered_sentence_hamlet))

data = normalize_text_emma
df_emma = pd.DataFrame([x.split('[') for x in data.split(']')])
df_emma.drop([2])

data = normalize_text_bible
df_bible = pd.DataFrame([x.split('[') for x in data.split(']')])
df_bible.drop([2])

data = normalize_text_alice
df_alice = pd.DataFrame([x.split('[') for x in data.split(']')])
df_alice.drop([2])

data = normalize_text_moby
df_moby = pd.DataFrame([x.split('[') for x in data.split(']')])
df_moby.drop([2])

data = normalize_text_caesar
df_caesar = pd.DataFrame([x.split('[') for x in data.split(']')])
df_caesar.drop([2])

data = normalize_text_hamlet
df_hamlet = pd.DataFrame([x.split('[') for x in data.split(']')])
df_hamlet.drop([2])

df_freq_emma = df_emma.sort_values(by=1,ascending=False)
print("Termos mais frequentes Emma: ", df_freq_emma[:5])
print("-------------------------------------------------")
df_freq_bible = df_bible.sort_values(by=1,ascending=False)
print("Termos mais frequentes Bible: ", df_freq_bible[:5])
print("-------------------------------------------------")
df_freq_alice = df_bible.sort_values(by=1,ascending=False)
print("Termos mais frequentes Alice: ", df_freq_alice[:5])
print("-------------------------------------------------")
df_freq_moby = df_moby.sort_values(by=1,ascending=False)
print("Termos mais frequentes Moby: ", df_freq_moby[:5])
print("-------------------------------------------------")
df_freq_caesar = df_caesar.sort_values(by=1,ascending=False)
print("Termos mais frequentes Caesar: ", df_freq_caesar[:5])
print("-------------------------------------------------")
df_freq_hamlet = df_hamlet.sort_values(by=1,ascending=False)
print("Termos mais frequentes Hamlet: ", df_freq_hamlet[:5])

#Inverse Document Frequency

N = df_emma.count()
#idf(t) = N/df