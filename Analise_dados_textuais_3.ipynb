{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analise_dados_textuais_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Z_nRqHSPGQ"
      },
      "source": [
        "**Ernesto Gurgel Valente Neto**\n",
        "\n",
        "---\n",
        "# **Materia: Analise de Dados Textuais**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Prof: Wellington"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ICom4eISOnc",
        "outputId": "f41049ad-f57a-48d7-c6ab-a34bbd436394"
      },
      "source": [
        "print('Importação de Pacotes')\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importação de Pacotes\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2_C63CUSiHx"
      },
      "source": [
        "1) \n",
        "# **Utililizando as técnicas aprendidas sobre Term Frequency, Inverse Document Frequency, liste as 5 palavras mais relevantes de cada texto contido no corpus Gutemberg. A sua analise deve ser em cima dos seguintes textos : 'austen-emma.txt', 'bible-kjv.txt', 'carroll-alice.txt', 'melville-moby_dick.txt', 'shakespeare-caesar.txt' e 'shakespeare-hamlet.txt'. Lembre-se que antes de aplicar o as tecnicas de TF IDF o texto deve está normalizado, com as stopwords retiradas e lematizado.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAMT0pSgSvGe",
        "outputId": "0eead88e-b7b0-4106-9647-74a6a0926b53"
      },
      "source": [
        "#nltk.corpus.gutenberg.fileids()\n",
        "#Importando as bibliotecas\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import gutenberg\n",
        "#lista_stop = stopwords.words()\n",
        "lista_stop = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE4KFieyWFfM"
      },
      "source": [
        "#tratando os dados\n",
        "texto_hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "texto_hamlet = set(word.lower() for word in texto_hamlet if word.isalpha())\n",
        "texto_hamlet = [nltk.word_tokenize(str(comment)) for comment in texto_hamlet]\n",
        "\n",
        "lista_stop\n",
        "filtered_sentence_hamlet = []\n",
        "for word in texto_hamlet:\n",
        "    if word not in lista_stop:\n",
        "        filtered_sentence_hamlet.append(word)\n",
        "\n",
        "#tratando os dados\n",
        "texto_emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "texto_emma = set(word.lower() for word in texto_emma if word.isalpha())\n",
        "texto_emma = [nltk.word_tokenize(str(comment)) for comment in texto_emma]\n",
        "\n",
        "lista_stop\n",
        "filtered_sentence_emma = []\n",
        "for word in texto_emma:\n",
        "    if word not in lista_stop:\n",
        "        filtered_sentence_emma.append(word)      \n",
        "\n",
        "#tratando os dados\n",
        "texto_bible = nltk.corpus.gutenberg.words('bible-kjv.txt')\n",
        "texto_bible = set(word.lower() for word in texto_bible if word.isalpha())\n",
        "texto_bible = [nltk.word_tokenize(str(comment)) for comment in texto_bible]\n",
        "\n",
        "lista_stop\n",
        "filtered_sentence_bible = []\n",
        "for word in texto_bible:\n",
        "    if word not in lista_stop:\n",
        "        filtered_sentence_bible.append(word)\n",
        "\n",
        "#tratando os dados\n",
        "texto_alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
        "texto_alice = set(word.lower() for word in texto_alice if word.isalpha())\n",
        "texto_alice = [nltk.word_tokenize(str(comment)) for comment in texto_alice]\n",
        "\n",
        "lista_stop\n",
        "filtered_sentence_alice = []\n",
        "for word in texto_alice:\n",
        "    if word not in lista_stop:\n",
        "        filtered_sentence_alice.append(word)\n",
        "\n",
        "#tratando os dados\n",
        "texto_moby = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
        "texto_moby = set(word.lower() for word in texto_moby if word.isalpha())\n",
        "texto_moby = [nltk.word_tokenize(str(comment)) for comment in texto_moby]\n",
        "\n",
        "lista_stop\n",
        "filtered_sentence_moby = []\n",
        "for word in texto_moby:\n",
        "    if word not in lista_stop:\n",
        "        filtered_sentence_moby.append(word)\n",
        "\n",
        "#tratando os dados\n",
        "texto_caesar = nltk.corpus.gutenberg.words('shakespeare-caesar.txt')\n",
        "texto_caesar = set(word.lower() for word in texto_caesar if word.isalpha())\n",
        "texto_caesar = [nltk.word_tokenize(str(comment)) for comment in texto_caesar]\n",
        "\n",
        "lista_stop\n",
        "filtered_sentence_caesar = []\n",
        "for word in texto_caesar:\n",
        "    if word not in lista_stop:\n",
        "        filtered_sentence_caesar.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2R9Ku9IUV9S"
      },
      "source": [
        "#importando bibliotecas\n",
        "import pandas as pd\n",
        "import altair as alt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIuIJnphUYcx"
      },
      "source": [
        "dfemma = filtered_sentence_emma\n",
        "dfbiblia = filtered_sentence_bible\n",
        "dfalice = filtered_sentence_alice\n",
        "dfmoby = filtered_sentence_moby\n",
        "dfcaesar = filtered_sentence_caesar\n",
        "dfhamlet = filtered_sentence_hamlet\n",
        "#print(\"Emma: \", len(dfemma), \"/Biblia: \", len(dfbiblia), \"/Alice: \",len(dfalice), \"/Moby :\", len(dfmoby), \"/Caesar: \", len(dfcaesar), \"/Hammlet: \", len(dfhamlet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bj1PVsgfBaA"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVyak3_OVn5W"
      },
      "source": [
        "def normalize_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = txt.replace(',','')\n",
        "    txt = txt.replace(';','')\n",
        "    txt = txt.replace('.','')\n",
        "    txt = txt.replace('\\'','')\n",
        "    txt = txt.replace('(','')\n",
        "    txt = txt.replace(')','')\n",
        "    txt = txt.replace(':','')\n",
        "    txt = txt.replace('!','')\n",
        "    txt = txt.replace('?','')\n",
        "    txt = txt.replace(\"\\\\\",\"\")\n",
        "    txt = txt.replace(\"\\\"\",\"\")\n",
        "    txt = txt.replace(\"`\",\"\")\n",
        "    txt = txt.replace('</p>','')\n",
        "    return txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvQ69VNCVtRd"
      },
      "source": [
        "#Removendo caracteres especiais\n",
        "normalize_text_emma = normalize_text(str(filtered_sentence_emma))\n",
        "normalize_text_bible = normalize_text(str(filtered_sentence_bible))\n",
        "normalize_text_alice = normalize_text(str(filtered_sentence_alice))\n",
        "normalize_text_moby = normalize_text(str(filtered_sentence_moby))\n",
        "normalize_text_caesar = normalize_text(str(filtered_sentence_caesar))\n",
        "normalize_text_hamlet = normalize_text(str(filtered_sentence_hamlet))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "JIJVoDgVjDIz",
        "outputId": "14e7db2e-faa8-4c70-bce8-81ede10d9721"
      },
      "source": [
        "data = normalize_text_emma\n",
        "df_emma = pd.DataFrame([x.split('[') for x in data.split(']')])\n",
        "df_emma.drop([2])\n",
        "\n",
        "data = normalize_text_bible\n",
        "df_bible = pd.DataFrame([x.split('[') for x in data.split(']')])\n",
        "df_bible.drop([2])\n",
        "\n",
        "data = normalize_text_alice\n",
        "df_alice = pd.DataFrame([x.split('[') for x in data.split(']')])\n",
        "df_alice.drop([2])\n",
        "\n",
        "data = normalize_text_moby\n",
        "df_moby = pd.DataFrame([x.split('[') for x in data.split(']')])\n",
        "df_moby.drop([2])\n",
        "\n",
        "data = normalize_text_caesar\n",
        "df_caesar = pd.DataFrame([x.split('[') for x in data.split(']')])\n",
        "df_caesar.drop([2])\n",
        "\n",
        "data = normalize_text_hamlet\n",
        "df_hamlet = pd.DataFrame([x.split('[') for x in data.split(']')])\n",
        "df_hamlet.drop([2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>aboue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>visage</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td>galls</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>fiers</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td></td>\n",
              "      <td>besides</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4696</th>\n",
              "      <td></td>\n",
              "      <td>performe</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4697</th>\n",
              "      <td></td>\n",
              "      <td>vnreclaim</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4698</th>\n",
              "      <td></td>\n",
              "      <td>sollemne</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4699</th>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4700</th>\n",
              "      <td></td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4700 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0          1      2\n",
              "0                   aboue\n",
              "1           visage   None\n",
              "3            galls   None\n",
              "4            fiers   None\n",
              "5          besides   None\n",
              "...  ..        ...    ...\n",
              "4696      performe   None\n",
              "4697     vnreclaim   None\n",
              "4698      sollemne   None\n",
              "4699          None   None\n",
              "4700          None   None\n",
              "\n",
              "[4700 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5nkn87sgXki",
        "outputId": "dcbbea67-465c-4ba1-e3d1-40aa1b467156"
      },
      "source": [
        "df_freq_emma = df_emma.sort_values(by=1,ascending=False)\n",
        "print(\"Termos mais frequentes Emma: \", df_freq_emma[:5])\n",
        "print(\"-------------------------------------------------\")\n",
        "df_freq_bible = df_bible.sort_values(by=1,ascending=False)\n",
        "print(\"Termos mais frequentes Bible: \", df_freq_bible[:5])\n",
        "print(\"-------------------------------------------------\")\n",
        "df_freq_alice = df_bible.sort_values(by=1,ascending=False)\n",
        "print(\"Termos mais frequentes Alice: \", df_freq_alice[:5])\n",
        "print(\"-------------------------------------------------\")\n",
        "df_freq_moby = df_moby.sort_values(by=1,ascending=False)\n",
        "print(\"Termos mais frequentes Moby: \", df_freq_moby[:5])\n",
        "print(\"-------------------------------------------------\")\n",
        "df_freq_caesar = df_caesar.sort_values(by=1,ascending=False)\n",
        "print(\"Termos mais frequentes Caesar: \", df_freq_caesar[:5])\n",
        "print(\"-------------------------------------------------\")\n",
        "df_freq_hamlet = df_hamlet.sort_values(by=1,ascending=False)\n",
        "print(\"Termos mais frequentes Hamlet: \", df_freq_hamlet[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Termos mais frequentes Emma:        0         1     2\n",
            "492       zigzags  None\n",
            "3100         zeal  None\n",
            "3112     youthful  None\n",
            "626         youth  None\n",
            "6591     yourself  None\n",
            "-------------------------------------------------\n",
            "Termos mais frequentes Bible:         0            1     2\n",
            "3921           zuzims  None\n",
            "6238      zurishaddai  None\n",
            "6672           zuriel  None\n",
            "11567             zur  None\n",
            "7913             zuph  None\n",
            "-------------------------------------------------\n",
            "Termos mais frequentes Alice:         0            1     2\n",
            "3921           zuzims  None\n",
            "6238      zurishaddai  None\n",
            "6672           zuriel  None\n",
            "11567             zur  None\n",
            "7913             zuph  None\n",
            "-------------------------------------------------\n",
            "Termos mais frequentes Moby:         0          1     2\n",
            "16758     zoroaster  None\n",
            "10050       zoology  None\n",
            "2268          zones  None\n",
            "11400         zoned  None\n",
            "9045           zone  None\n",
            "-------------------------------------------------\n",
            "Termos mais frequentes Caesar:        0          1     2\n",
            "488         youths  None\n",
            "1325     youthfull  None\n",
            "2645         yours  None\n",
            "944           your  None\n",
            "1815         young  None\n",
            "-------------------------------------------------\n",
            "Termos mais frequentes Hamlet:        0      1     2\n",
            "676       zone  None\n",
            "1689     youth  None\n",
            "4176     yours  None\n",
            "1455      your  None\n",
            "2873     young  None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCi6Mk6YwTnX"
      },
      "source": [
        "#Inverse Document Frequency\n",
        "\n",
        "N = df_emma.count()\n",
        "#idf(t) = N/df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}