# -*- coding: utf-8 -*-
"""Analise_dados_textuais_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mWrOrdE4of0psj69fSBsQpmowVye1Sn1

# **Ernesto Gurgel Valente Neto**

---
# **Materia: Analise de Dados Textuais**


---

Prof: Wellington
"""

print('Importação de Pacotes')
import nltk
import re
nltk.download('gutenberg')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

nltk.corpus.gutenberg.fileids()

"""# **# 1) Retire as stopwords dos seguintes textos (Esses textos pertencem ao corpus gutemberg):**


"""

#Importando as bibliotecas
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize  
from nltk.tokenize import word_tokenize
nltk.download('stopwords')

#lista_stop = stopwords.words()
lista_stop = nltk.corpus.stopwords.words('english')

"""**a) shakespeare-caesar.txt**



"""

texto = nltk.corpus.gutenberg.words('shakespeare-caesar.txt')
texto = set(word.lower() for word in texto if word.isalpha())
texto = [nltk.word_tokenize(str(comment)) for comment in texto]

lista_stop
filtered_sentence = []
for word in texto:
    if word not in lista_stop:
        filtered_sentence.append(word)

print("Lista Stopwords", lista_stop)
print(filtered_sentence)

"""**b) shakespeare-hamlet.txt**"""

texto2 = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')
texto2 = set(word.lower() for word in texto2 if word.isalpha())
texto2 = [nltk.word_tokenize(str(comment)) for comment in texto2]

lista_stop
filtered_sentence2 = []
for word in texto2:
    if word not in lista_stop:
        filtered_sentence2.append(word)

print("Lista Stopwords", lista_stop)
print(filtered_sentence2)

"""# **# 2) Qual é a quantidade de palavras restantes em cada texto?**"""

print("Quantidade de Palavras Restantes em Caesar:", len(filtered_sentence))
print("Quantidade de Palavras Restantes em Hamlet:", len(filtered_sentence2))

"""# **# 3) Encontre as entidades nomeada presentes para o seguintes textos:**"""

#Preparando toda a base de dados para leitura
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
from nltk import tokenize

#Abrindo arquivos_Preparando
apoloxi = open('apoloxi.txt').read()
apoloxi_tokenize = tokenize.word_tokenize(apoloxi, language='english')
apoloxi_tag = nltk.pos_tag(apoloxi_tokenize)

french_revolution = open('french-revolution.txt').read()
french_revolution_tokenize = tokenize.word_tokenize(french_revolution, language='english')
french_revolution_tag = nltk.pos_tag(french_revolution_tokenize)



"""a) **apoloxi.txt**"""

apoloxi_chunk = nltk.ne_chunk(apoloxi_tag)
print("Entidades encontradas em Apoloxi:", apoloxi_chunk[0:100])

"""**b) french-revolution.txt**


"""

french_revolution_chunk = nltk.ne_chunk(french_revolution_tag)
print("Entidades encontradas em French Revolution:", french_revolution_chunk[0:100])

"""# **# 4) Qual é a quantidade de entidades "GPE" presentes em cada um dos textos?**"""

contador_GPE_Apoloxi = 0
for entidade_nomeadas in apoloxi_chunk:
  if hasattr(entidade_nomeadas, 'label'):
    if not entidade_nomeadas.label() =='GPE':
      contador_GPE_Apoloxi = contador_GPE_Apoloxi + 1

contador_GPE_FrenchRevolution = 0
for entidade_nomeadas in french_revolution_chunk:
  if hasattr(entidade_nomeadas, 'label'):
    if not entidade_nomeadas.label() =='GPE':
      contador_GPE_FrenchRevolution = contador_GPE_FrenchRevolution + 1

print("Quantidade de GPE encontrada no texto Apoloxi: ", contador_GPE_Apoloxi)
print("Quantidade de GPE encontrada no texto French Revolution: ", contador_GPE_FrenchRevolution)

"""# **5) Qual é a quantidade de entidades "LOCATION" presentes em cada um dos textos?**"""

contador_LOCATION_APOLOXI = 0
for entidade_nomeadas in apoloxi_chunk:
  if hasattr(entidade_nomeadas, 'label'):
    if not entidade_nomeadas.label() =='LOCATION':
      contador_LOCATION_APOLOXI += 1

contador_LOCATION_FRENCH_REVOLUTION = 0
for entidade_nomeadas in french_revolution_chunk:
  if hasattr(entidade_nomeadas, 'label'):
    if not entidade_nomeadas.label() =='LOCATION':
      contador_LOCATION_FRENCH_REVOLUTION += 1

print("Quantidade de GPE encontrada no texto Apoloxi: ", contador_LOCATION_APOLOXI)
print("Quantidade de GPE encontrada no texto French Revolution: ", contador_LOCATION_FRENCH_REVOLUTION)

"""# **6) Qual é a quantidade de entidades "PERSON" presentes em cada um dos textos?**"""

contador_PERSON_APOLOXI = 0
for entidade_nomeadas in apoloxi_chunk:
  if hasattr(entidade_nomeadas, 'label'):
    if not entidade_nomeadas.label() =='PERSON':
      contador_PERSON_APOLOXI += 1

contador_PERSON_FRENCH = 0
for entidade_nomeadas in french_revolution_chunk:
  if hasattr(entidade_nomeadas, 'label'):
    if not entidade_nomeadas.label() =='PERSON':
      contador_PERSON_FRENCH += 1

print("Quantidade de GPE encontrada no texto Apoloxi: ", contador_PERSON_APOLOXI)
print("Quantidade de GPE encontrada no texto French Revolution: ", contador_PERSON_FRENCH)