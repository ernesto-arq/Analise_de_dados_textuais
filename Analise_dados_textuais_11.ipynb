{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analise_dados_textuais_11.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IcWSlItnZrLa",
        "dS3oDRW_jzNp",
        "GhDuNEbsjkBB",
        "RGaf2bXKl0TW"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Ild1Ndl4Ar"
      },
      "source": [
        "Nome: Ernesto Gurgel Valente Neto\n",
        "\n",
        "> Disciplina: Analise de Dados Textuais"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvnI1MKgTgom",
        "outputId": "8ed5e00a-4d35-499d-b18c-0ef3523ae53f"
      },
      "source": [
        "import nltk\n",
        "import math\n",
        "import numpy\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReUe7k8GUEaD"
      },
      "source": [
        "# **Importando os arquivos de trabalho**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlgiFv6uTz7t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "b138b52f-4513-40a0-c52f-1fbc5bbec7c1"
      },
      "source": [
        "dataset = {\n",
        "    \"tfidf_1.txt\":open(\"tfidf_1.txt\").read(),\n",
        "    \"tfidf_2.txt\":open(\"tfidf_2.txt\").read(),\n",
        "    \"tfidf_3.txt\":open(\"tfidf_3.txt\").read(),\n",
        "    \"tfidf_4.txt\":open(\"tfidf_4.txt\").read(),\n",
        "    \"tfidf_5.txt\":open(\"tfidf_5.txt\").read(),\n",
        "    \"tfidf_6.txt\":open(\"tfidf_6.txt\").read(),\n",
        "    \"tfidf_7.txt\":open(\"tfidf_7.txt\").read(),\n",
        "    \"tfidf_8.txt\":open(\"tfidf_8.txt\").read(),\n",
        "    \"tfidf_9.txt\":open(\"tfidf_9.txt\").read(),\n",
        "    \"tfidf_10.txt\":open(\"tfidf_10.txt\").read() }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e88291119db2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m\"tfidf_1.txt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf_1.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"tfidf_2.txt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf_2.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"tfidf_3.txt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf_3.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"tfidf_4.txt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf_4.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tfidf_1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QfJnQRPVLJI"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ9C8ILyVNoH"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe0UBnLxVO6I"
      },
      "source": [
        "dataset['tfidf_1.txt']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C7h3yMXVXW4"
      },
      "source": [
        "# **Criar uma frequencia de termos em documento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyrQzisqVgYl"
      },
      "source": [
        "def tf(dataset, file_name):\n",
        "    text = dataset[file_name]\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    fd = nltk.FreqDist(tokens)\n",
        "    return fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOpNCcziV53U"
      },
      "source": [
        "# **Função apra calcular o inverso da frequencia do termo de cada docuemtno**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPHd4FaLV3Om"
      },
      "source": [
        "# Calcular o inverso da frequencia de um termo em determinado documento\n",
        "def idf(dataset, term):\n",
        "    count = [term in dataset[file_name] for file_name in dataset]\n",
        "    inv_df = math.log(len(count)/sum(count))\n",
        "    return inv_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95sm-MXUWVmv"
      },
      "source": [
        "# **Criar a função do Term Frequenci, Inverse Documetn Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHKQ8VH5WTZ2"
      },
      "source": [
        "def tfidf(dataset, file_name, n):\n",
        "    term_scores = {}\n",
        "    file_fd = tf(dataset,file_name)\n",
        "    for term in file_fd:\n",
        "        if term.isalpha():\n",
        "            idf_val = idf(dataset,term)\n",
        "            tf_val = tf(dataset, file_name)[term]\n",
        "            tfidf = tf_val*idf_val\n",
        "            term_scores[term] = round(tfidf,2)\n",
        "    return sorted(term_scores.items(), key=lambda x:-x[1])[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYQF5AmPWum7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "n461lhx5WuuE",
        "outputId": "ce47f237-ec95-40fd-ec4a-f322c33e5b06"
      },
      "source": [
        "tfidf(dataset,\"tfidf_1.txt\",1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d50c7fd0d9b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tfidf_1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw1O7AfEYKFW"
      },
      "source": [
        "tfidf(dataset,\"tfidf_1.txt\",10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhPRNeLTYOZV"
      },
      "source": [
        "tfidf(dataset,\"tfidf_1.txt\",10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28ffgsfDYQyO"
      },
      "source": [
        "for file_name in dataset:\n",
        "    print(\"{0}: \\n {1} \\n\".format(file_name, tfidf(dataset,file_name,5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcWSlItnZrLa"
      },
      "source": [
        "# Similaridade de textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrutKiAeZv35"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHm4G44LdjjG"
      },
      "source": [
        "#nltk.corpus.stopwords.words(\"portuguese\")\n",
        "#ja incluso no pacote"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn9cYNNvdvpz"
      },
      "source": [
        "extracao de palavras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VvdREgkdxya"
      },
      "source": [
        "#funcao ajuda nos trabalhos\n",
        "def word_extraction(sentence):    \n",
        "  ignore = nltk.corpus.stopwords.words(\"english\") \n",
        "  words = nltk.word_tokenize(sentence)  \n",
        "  cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
        "  return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xdxkes0d70D"
      },
      "source": [
        "#normalizando todas as sentenças em letra minuscula\n",
        "def word_normalize(sentence):\n",
        "  return [word.lower() for word in sentence if word.isalpha()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGHDBR1oeZES"
      },
      "source": [
        "#quebrar as frases em sentensas processo de tokenização\n",
        "def tokenize(sentence):\n",
        "  tokens = nltk.word_tokenize(sentence)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET0DgOo5eJ5L"
      },
      "source": [
        "#tokenizar as sentenças, uma lista com as palavras tokenizadas\n",
        "#imoporntate para montar vetores de palavras e elas estarao alinhadas novetor em questao da similaridade\n",
        "def tokenize_sent(sentences):    \n",
        "  words = []    \n",
        "  for sentence in sentences:       \n",
        "    w = word_extraction(sentence)        \n",
        "    words.extend(w)            \n",
        "    words = sorted(list(set(words)))    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tXin5lCfuhh"
      },
      "source": [
        "def generate_bow(allsentences):        \n",
        "  vocab = tokenize(allsentences)    \n",
        "  print(\"Word List for Document \\n{0} \\n\".format(vocab));\n",
        "  for sentence in allsentences:        \n",
        "    words = word_normalize(sentence)        \n",
        "    bag_vector = numpy.zeros(len(vocab))        \n",
        "    for w in words:            \n",
        "      for i,word in enumerate(vocab):                \n",
        "        if word == w:                    \n",
        "          bag_vector[i] += 1                           \n",
        "          print(\"{0}\\n{1}\\n\".format(sentence,numpy.array(bag_vector)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAssoTmNhKNV"
      },
      "source": [
        "allsentences1 = [\"Joe waited for the train train\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uee85lIzhR5Z"
      },
      "source": [
        "generate_bow(str(allsentences1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz9gyc8xhNyx"
      },
      "source": [
        "allsentences2 = [\"Joe waited for the train train\", \"The train was late\", \"Mary and Samantha took the bus\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdbIc5gShKrQ"
      },
      "source": [
        "generate_bow(str(allsentences2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS3oDRW_jzNp"
      },
      "source": [
        "# **Utilizando a funcao do Sklearn para fazer as mesmas coisas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9NLTxGOhbFy"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN8-lM8Zi8dv"
      },
      "source": [
        "corpus = [\n",
        "'All my cats in a row',\n",
        "'When my cat sits down, she looks like a Furby toy!',\n",
        "'The cat from outer space',\n",
        "'Sunshine loves to sit like this for some reason.'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZHSRQ8ui-Qs"
      },
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-FeHD3Gi_j-"
      },
      "source": [
        "print(vectorizer.fit_transform(corpus).todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAMKsygijAyb"
      },
      "source": [
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpErsjQFgQ11"
      },
      "source": [
        "*Distância* Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wYIQv38jErL"
      },
      "source": [
        "#poderiamos normalizar ou tokenizar, visto que essa funcao so compara palavras iguais\n",
        "def get_jaccard_sim(str1, str2): \n",
        "    a = set(str1.split()) \n",
        "    b = set(str2.split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_wwLjfrjGQ_"
      },
      "source": [
        "get_jaccard_sim(\"All my cat in a row\", \"The cats from outer space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUmd-y0RjR-W"
      },
      "source": [
        "def cos_sim(a, b):\n",
        "\tdot_product = np.dot(a, b)\n",
        "\tnorm_a = np.linalg.norm(a)\n",
        "\tnorm_b = np.linalg.norm(b)\n",
        "\treturn dot_product / (norm_a * norm_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u8gOF-6ja7K"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11LUm8m5jcGT"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PjolFwtjhgr"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhDuNEbsjkBB"
      },
      "source": [
        "# **Distancia Cosseno**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0_r9F1fjtea"
      },
      "source": [
        "def get_cosine_sim(*strs): \n",
        "    vectors = [t for t in get_vectors(*strs)]\n",
        "    return cosine_similarity(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12Zo7nu4kD6p"
      },
      "source": [
        "def get_vectors(*strs):\n",
        "    text = [t for t in strs]\n",
        "    vectorizer = CountVectorizer(text)\n",
        "    vectorizer.fit(text)\n",
        "    return vectorizer.transform(text).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTxXf79okKzZ"
      },
      "source": [
        "get_vectors(\"All my cat in a row\", \"The cats from outer space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVuxcGiMkWqQ"
      },
      "source": [
        "#Comparando duas sentenças\n",
        "get_cosine_sim(\"All my cat in a row\", \"The cats from outer space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QljgeLsblK_v"
      },
      "source": [
        "#Comparando duas sentenças\n",
        "lista = get_cosine_sim(\"All my cat in a row\", \"The cats from outer space\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmC5XznvlMNe"
      },
      "source": [
        "#Comparando duas sentenças\n",
        "lista[0:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGaf2bXKl0TW"
      },
      "source": [
        "# **Fim da lista**"
      ]
    }
  ]
}