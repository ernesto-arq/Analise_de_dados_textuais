# -*- coding: utf-8 -*-
"""Analise_dados_textuais_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mreEDBlQwOA8cZ5SYCUJQ9xO2eiSCua3
"""

import nltk

nltk.download('gutenberg')

alice = nltk.corpus.gutenberg.words('carroll-alice.txt')

alice

alice_freq = nltk.FreqDist(alice)

alice_freq

alice_100 = alice_freq.most_common(100)

len(alice_100)

alice_100

"""Normalizar os dados"""

alice_common = []

for word in alice_100:
  alice_common.append(word[0])

# alice_common = [word[0] for word in alice_100]

alice_common

common = set(word.lower() for word in alice_common if word.isalpha())

common

len(common)

"""Tratando as StopWords"""

nltk.download('stopwords')

stopwords = nltk.corpus.stopwords.words('english')

stopwords

len(stopwords)

conjunto_final = list(set(common) - set(stopwords))

conjunto_final

len(conjunto_final)

"""Incluir um único arquivo"""

from google.colab import files

uploaded = files.upload()

uploaded = files.upload()

from google.colab import drive

drive.mount('/content/drive')

"""Leitura de arquivos"""

text = open("exemplo.txt").read()

text

nltk.download('punkt')

nltk.download('averaged_perceptron_tagger')

text_tag = nltk.pos_tag(nltk.word_tokenize(text))

text_tag

nltk.download('maxent_ne_chunker')

nltk.download('words')

text_ch = nltk.ne_chunk(text_tag)

text_ch[:20]

len(text_ch)

for entidade_nomeada in text_ch:
   if hasattr(entidade_nomeada, 'label'):
       print(entidade_nomeada.label(), ' '.join(c[0] 
              for c in entidade_nomeada.leaves()))

"""Tratar paginas web"""

import urllib.request

"""Os sites são escritos em HTML; portanto, quando você extrai informações diretamente de um site, você recebe todo o código de volta junto com o texto."""

url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'

url

"""Fazendo a requisição na pagina"""

response = urllib.request.urlopen(url)

"""Lendo a informação"""

html = response.read()

html[:1000]

"""Usaremos uma biblioteca Python chamada BeautifulSoup para facilitar o trabalho com o HTML."""

from bs4 import BeautifulSoup

soup = BeautifulSoup(html, "html5lib")

"""A Wikipedia coloca o texto mais legível nas tags de parágrafo ou "p"."""

web_paragraph = []

for p_tag in soup.find_all('p'):
  web_paragraph.append(p_tag.text)

web_paragraph

len(web_paragraph)

web_tokens = [nltk.word_tokenize(paragraph) for paragraph in web_paragraph]

web_tokens[2]

comments = []

import csv

with open("reviews.csv", "r") as file:
  reader = csv.reader(file)
  for row in reader:
        comments.append(row)

comments[0]

len(comments)

tokens = [nltk.word_tokenize(str(comment)) for comment in comments]

tokens[0]

import numpy as np

negative = []

with open("negative_words.csv", "r") as file:
  reader = csv.reader(file)
  for row in reader:
        negative.append(row)

negative

positive = []

with open("positive_words.csv", "r") as file:
  reader = csv.reader(file)
  for row in reader:
        positive.append(row)

positive[:50]

"""Vamos criar uma função para marcar os sentimentos das sentenças

*   Para cada palavra positiva no texto adicionamos (+1) em um contador positivo
*   Para cada palavra negativa no texto adicionamos (+1) em um contador negativo

Vamos usar a lógica de par.
"""

def sentiment(text):
    temp = [] #
    #vamos "quebrar" o texto em sentenças
    text_sent = nltk.sent_tokenize(text)
    for sentence in text_sent:
        n_count = 0
        p_count = 0
        #Para cada sentença vamos quebrar em palavras/tokens
        sent_words = nltk.word_tokenize(sentence)
        #se a palavra estiver na lista de positiva adicionamos +1 se estiver na negativa adicionamos +1
        for word in sent_words:
            for item in positive:
                if(word == item[0]):
                    p_count +=1
            for item in negative:
                if(word == item[0]):
                    n_count +=1
        #vamos agora verificar alguns casos
        #[Caso 1: qualquer número de apenas positivos(+)]
        if(p_count > 0 and n_count == 0): #any number of only positives (+) [case 1]
            #print "+ : " + sentence
            temp.append(1)
         #[Caso 2: Vou usar impar para saber se é negativos(+)]
        elif(n_count%2 > 0): #odd number of negatives (-) [case2]
            #print "- : " + sentence
            temp.append(-1)
         #[Caso 3: Pares negativos(+)]   
        elif(n_count%2 ==0 and n_count > 0): #even number of negatives (+) [case3]
            #print "+ : " + sentence
            temp.append(1)
        #[Caso 4: Neutro]    
        else:
            #print "? : " + sentence
            temp.append(0)
    return temp

sentiment("It was terribly bad.")

sentiment("Actualluty, it was not bad at all.")

sentiment("This is a sentance about nothing.")

my_list = sentiment("I saw this movie the other night. I can say I was not disappointed! The actiing and story line was amazing and kept me on the edge of my seat the entire time. While I did not care for the music, it did not take away from the overall experience. I would highly recommend this movie to anyone who enjoys thirllers.")

my_list

comments

for review in comments:
  print('\n')
  print(np.average(sentiment(str(review))))
  print(review)

